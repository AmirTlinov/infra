#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import re
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path


TOKEN_RE = re.compile(r"^[A-Z][A-Z0-9_/-]*$")
TOKEN_REF_RE = re.compile(r"\[([^\[\]]+)\]")


@dataclass(frozen=True)
class DocBlocks:
    legend: str
    content: str


def _repo_root() -> Path:
    return Path(__file__).resolve().parents[1]


def _strip_inline_code(text: str) -> str:
    parts = text.split("`")
    if len(parts) < 3:
        return text
    out: list[str] = []
    for i, part in enumerate(parts):
        if i % 2 == 0:
            out.append(part)
    return "".join(out)


def _parse_blocks(md: str) -> DocBlocks:
    lines = md.splitlines()
    i = 0
    while i < len(lines) and not lines[i].strip():
        i += 1
    if i >= len(lines) or lines[i].strip() != "[LEGEND]":
        raise ValueError("first non-empty line must be [LEGEND]")

    legend_start = i + 1
    content_header_idx = None
    in_fence = False
    for j in range(legend_start, len(lines)):
        line = lines[j]
        if line.strip().startswith("```"):
            in_fence = not in_fence
            continue
        if in_fence:
            continue
        if line.strip() == "[CONTENT]":
            content_header_idx = j
            break

    if content_header_idx is None:
        raise ValueError("missing [CONTENT] header")

    legend = "\n".join(lines[legend_start:content_header_idx])
    content = "\n".join(lines[content_header_idx + 1 :])
    return DocBlocks(legend=legend, content=content)


def _parse_legend_defs(legend: str, source: str) -> dict[str, str]:
    defs: dict[str, str] = {}
    for raw in legend.splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if "=" not in line:
            continue
        token, meaning = line.split("=", 1)
        token = token.strip()
        meaning = meaning.strip()
        if not TOKEN_RE.match(token):
            continue
        if not meaning:
            raise ValueError(f"legend token {token} in {source} must have meaning")
        defs[token] = meaning
    return defs


def _parse_content_refs(content: str) -> list[str]:
    tokens: set[str] = set()
    in_fence = False
    for raw in content.splitlines():
        line = raw
        if line.strip().startswith("```"):
            in_fence = not in_fence
            continue
        if in_fence:
            continue
        line = _strip_inline_code(line)
        for hit in TOKEN_REF_RE.findall(line):
            token = hit.split("|", 1)[0].strip()
            if TOKEN_RE.match(token):
                tokens.add(token)
    return sorted(tokens)


def _doc_entry(kind: str, path: Path, root: Path) -> dict[str, str]:
    return {"kind": kind, "path": path.relative_to(root).as_posix()}


def _collect_docs(root: Path) -> list[dict[str, str]]:
    docs: list[dict[str, str]] = []
    context = ["ARCHITECTURE.md", "GOALS.md", "MAP.md", "PHILOSOPHY.md"]
    for name in context:
        path = root / name
        if path.exists():
            docs.append(_doc_entry("context", path, root))

    contracts_dir = root / "docs" / "contracts"
    if contracts_dir.exists():
        for path in sorted(contracts_dir.glob("*_v*.md")):
            docs.append(_doc_entry("contract", path, root))

    for name in ["CONTRACT_STANDARD.md", "README.md"]:
        path = contracts_dir / name
        if path.exists():
            docs.append(_doc_entry("contracts-meta", path, root))

    for name in ["AGENT_PLAYBOOK.md", "DOC_STYLE.md"]:
        path = root / "docs" / name
        if path.exists():
            docs.append(_doc_entry("doc", path, root))

    legend = root / "LEGEND.md"
    if legend.exists():
        docs.append(_doc_entry("legend", legend, root))

    return docs


def _load_existing(path: Path) -> dict | None:
    if not path.exists():
        return None
    return json.loads(path.read_text(encoding="utf-8"))


def _build_index(root: Path, generated_at: str | None) -> dict:
    docs = _collect_docs(root)
    refs: dict[str, list[str]] = {}
    definitions: dict[str, tuple[str, str]] = {}

    for entry in docs:
        doc_path = root / entry["path"]
        blocks = _parse_blocks(doc_path.read_text(encoding="utf-8"))
        refs[entry["path"]] = _parse_content_refs(blocks.content)
        for token, meaning in _parse_legend_defs(blocks.legend, entry["path"]).items():
            if token in definitions:
                raise ValueError(f"token {token} defined twice ({definitions[token][0]}, {entry['path']})")
            definitions[token] = (entry["path"], meaning)

    reverse_index: dict[str, dict[str, object]] = {}
    usage: dict[str, set[str]] = {}
    for doc_path, tokens in refs.items():
        for token in tokens:
            usage.setdefault(token, set()).add(doc_path)

    for token in sorted(definitions.keys()):
        defined_in, meaning = definitions[token]
        reverse_index[token] = {
            "defined_in": defined_in,
            "meaning": meaning,
            "used_in": sorted(usage.get(token, set())),
        }

    index = {
        "docs": docs,
        "generated_at_utc": generated_at
        or datetime.now(timezone.utc).replace(microsecond=0).isoformat(),
        "refs": refs,
        "reverse_index": reverse_index,
        "version": 1,
    }
    return index


def _same_without_timestamp(a: dict, b: dict) -> bool:
    a = dict(a)
    b = dict(b)
    a.pop("generated_at_utc", None)
    b.pop("generated_at_utc", None)
    return a == b


def main() -> int:
    parser = argparse.ArgumentParser(description="Refresh ai/context_index.json")
    parser.add_argument("--check", action="store_true", help="exit non-zero if changes are needed")
    args = parser.parse_args()

    root = _repo_root()
    output_path = root / "ai" / "context_index.json"
    existing = _load_existing(output_path) or {}
    generated_at = existing.get("generated_at_utc")
    index = _build_index(root, generated_at)

    if existing and not _same_without_timestamp(existing, index):
        index["generated_at_utc"] = datetime.now(timezone.utc).replace(microsecond=0).isoformat()

    if args.check:
        if not existing or not _same_without_timestamp(existing, index):
            return 2
        return 0

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(index, indent=2, sort_keys=False) + "\n", encoding="utf-8")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
